MCCD: Multi-Agent Collaboration-based Compositional Diffusion for
Complex Text-to-Image Generation
Mingcheng Li1,2* Xiaolu Hou1,2* Ziyang Liu3 Dingkang Yang1,2Â§ Ziyun Qian1,2
Jiawei Chen1,2 Jinjie Wei1,2 Yue Jiang1,2 Qingyao Xu1,2 Lihua Zhang1,2,4,5Â§
1Academy for Engineering and Technology, Fudan University 2Cognition and Intelligent Technology Laboratory (CIT Lab)
3School of Future Science and Engineering, Soochow University, Suzhou, China
4Jilin Provincial Key Laboratory of Intelligence Science and Engineering, Changchun, China
5Engineering Research Center of AI and Robotics, Ministry of Education, Shanghai, China
{mingchengli21, xlhou23}@m.fudan.edu.cn, dkyang20@fudan.edu.cn
Abstract
Diffusion models have shown excellent performance in text-
to-image generation. Nevertheless, existing methods often
suffer from performance bottlenecks when handling com-
plex prompts that involve multiple objects, characteris-
tics, and relations. Therefore, we propose a Multi-agent
Collaboration-based Compositional Diffusion (MCCD) for
text-to-image generation for complex scenes. Specifically,
we design a multi-agent collaboration-based scene parsing
module that generates an agent system comprising multiple
agents with distinct tasks, utilizing MLLMs to extract var-
ious scene elements effectively. In addition, Hierarchical
Compositional diffusion utilizes a Gaussian mask and fil-
tering to refine bounding box regions and enhance objects
through region enhancement, resulting in the accurate and
high-fidelity generation of complex scenes. Comprehensive
experiments demonstrate that our MCCD significantly im-
proves the performance of the baseline models in a training-
free manner, providing a substantial advantage in complex
scene generation.
1. Introduction
Recently, diffusion models [7, 30, 32, 38] have shown sig-
nificant advancements in Text-to-Image (T2I) generation,
such as Stable Diffusion [30], Imagen [31] and DALL-E
2/3 [2, 29]. However, despite their noteworthy performance
in generating realistic images consistent with text prompts,
these models have large limitations in processing complex
textual prompts and generating complex scenes, leading to
unsatisfactory image generation [10, 20, 23]. Therefore,
T2I models require powerful spatial perceptions that can
Â§Corresponding authors. *Equal contributions.
precisely align multiple objects with different attributes and
complex relationships involved in compositional prompts.
Some studies attempt to introduce additional conditions
to solve the problems above, which can be divided into two
parts: (i) spatial information-based methods [19, 20, 27, 41,
43], and (ii) feedback-based methods. Spatial information-
based methods utilize additional spatial information (e.g.,
layouts and boxes) as conditions to enhance the composi-
tionality of T2I generation. For example, GLIGEN [19]
introduces trainable gated self-attention layers to integrate
spatial inputs based on the pre-trained stable diffusion mod-
els. ReCo [41] utilizes additional sets of positional to-
kens for T2I generation to achieve effective region con-
trol and fine-tune the pre-trained T2I models. Feedback-
based methods [9, 15â€“17, 33, 36] use the generated images
as feedback to optimize the T2I generation. For instance,
DreamSync [33] utilizes a visual question answer model
and an aesthetic quality evaluation model to recognize fine-
grained discrepancies between the generated image and the
textual input, thus enhancing the semantic alignment ca-
pabilities of the T2I model. GORS [15] fine-tunes pre-
trained T2I models leveraging generated images aligned to
the text prompt and text-image alignment reward-weighted
loss. Parrot [17] jointly optimizes the T2I model with a
multi-reward optimization strategy to improve the quality
of image generation. However, the above methods suffer
from the following limitations: (i) lacking fine-grained and
precise spatial information guidance, resulting in unrealis-
tic spatial locations and relations in the generated images.
(ii) Difficulty in obtaining high-quality image feedback to
effectively optimize the image generation. (iii) Fine-tuning
of the T2I model ( e.g., stable diffusion) results in a large
amount of computational and time overheads.
To address the above-mentioned problems, we propose
a Multi-agent Collaboration-based Compositional Diffu-
1
arXiv:2505.02648v2  [cs.CV]  6 May 2025
sion (MCCD) for high-quality text-to-image generation
and complex scene generation. Our novelty stems from
three core contributions: (i) We propose a multi-agent
collaboration-based scene parsing module that constructs
multiple agents with different tasks to implement collabora-
tion in forward thought chain reasoning and backward feed-
back processes to precisely parse key scene elements. (ii)
Furthermore, Hierarchical Compositional diffusion is pro-
posed to achieve sufficient interaction among parsed multi-
ple scene elements and accurately generate complex scenes
that match the text prompt. (iii) Comprehensive qualitative
and quantitative experiments demonstrate that our method
significantly improves the performance of the baseline mod-
els in a training-free manner, which has large advantages.
2. Related Work
2.1. Text-to-Image Generation
Text-to-Image (T2I) generation, i.e., text-conditional image
synthesis, has been a key research hotspot in the field of
multimodal learning [2, 29]. Numerous efforts have been
devoted to generating visually natural and realistic images.
Generative Adversarial Networks (GANs) are typical T2I
models that utilize adversarial training between the genera-
tor and the discriminator to produce images that are as close
as possible to the real images. In recent years, inspired by
the application of Auto-Regressive Models (ARMs) in the
field of text generation, many works have achieved favor-
able results in the field of T2I generation utilizing ARMs,
such as CogView[8] and DALL-E 2/3 [2, 29]. Despite
the progress achieved by the above studies, they still have
many limitations, such as unstable training, difficult conver-
gence, and unidirectional bias, which lead to poorer qual-
ity of image generation and lower generalizability. Due to
the natural fit of the inductive bias to the image data, dif-
fusion models [7, 13, 30, 32, 38] are now widely used for
T2I generation and significantly improve image generation
quality and fidelity. GLIDE [25] utilizes the pre-trained
CLIP model [28] to achieve semantic alignment between
the text prompts and the generated images during the im-
age sampling process. Recent advances in T2I diffusion
models have significantly improved the quality and realism
of image generation in recent years, such as SDXL [26],
DALL-E 2/3 [2, 29] and ContextDiff [40]. In recent years,
Large Language Models (LLMs) have been widely used in
many tasks due to their powerful comprehension and rea-
soning capabilities [5, 6, 18, 34, 37, 45]. Many studies
use Multimodal Large Language Models (MLLMs) in text-
to-image (T2I) generation tasks and achieve performance
gains [11, 12, 14, 22, 27, 39, 44]. For example, RPG [39]
utilizes the Chain-of-Thought (CoT) of MLLMs to extract
layouts from text prompt to enhance T2I generation. LMD
[20] utilizes MLLMs to enhance the compositional gener-
ation of diffusion models by generating images grounded
on bounding box layouts from the MLLMs. However, the
above methods have the following limitations:(1) Simply
using MLLMs to process text prompts without sufficiently
exploiting and utilizing their powerful comprehension and
inference capabilities. (2) Only utilizing MLLMs as a lay-
out generator to control image synthesis, neglecting the ex-
traction of other important scene elements. To address the
above problems, we propose a Multi-agent Collaboration-
based scene Parsing (MCP) module, which constructs a
multi-agent system consisting of multiple agents with vari-
ous divisions of labor, and utilizes multi-agent collaboration
and interaction to achieve adequate extraction and parsing
of key scene elements in text prompt, thereby facilitating
the subsequent scene generation process.
2.2. Compositional Diffusion Generation
In recent years, many methods have been introduced to im-
prove compositional T2I generation [19, 20, 24, 41, 42] to
enhance the capabilities of diffusion models in terms of
attribute binding, object relations, and numeracy. For ex-
ample, ReCo [41] and GLIGEN [19] introduce a location-
aware adapter in diffusion models to improve the spatial
plausibility of the generated images. LMD [20] utilizes
LLM to generate scene layouts and designs a controller
to bootstrap the pre-trained diffusion model. RPG [39]
denoises each subregion in parallel and applies a post-
processing step of resizing and concatenation for high-
quality compositional generation. The T2I-Adapter [24]
facilitates compositional T2I generation by controlling the
semantic structure through some high-level features of
the image. However, these methods can only implement
coarse compositional control, leading to unsatisfactory im-
age generation results, especially when dealing with com-
plex prompts. Therefore, we propose hierarchical composi-
tional diffusion, which utilizes more precise control to pro-
gressively refine image synthesis.
3. Methodology
3.1. Overall Framework
As shown in Figure 1, given a complex text prompt con-
taining multiple objects and relations, the goal of MCCD is
to produce realistic and high-quality images. Our MCCD
is a training-free framework with the following workflow:
(i) Multi-agent collaboration-based scene parsing module
utilizes a multi-agent collaborative approach to parse indi-
vidual elements in a text prompt. (2) Hierarchical composi-
tional diffusion is used to interact with scene elements and
achieve high-quality image generation using dynamic inte-
gration, regional enhancement, and latent space smoothing.
2
Prompt:
The dining area is filled 
with natural light. A 
silver refrigerator stands 
nearby, while a large 
window brightens the 
space and â€¦
Input complex prompt
ğ‘ğ‘
Objects and
characteristics
prompt {ğ‘ğ‘œğ‘–}ğ‘–=0
ğ‘›âˆ’1
Background prompt
ğ‘ğ‘
Prompt Set
Bounding Boxes
{ğµğ‘–}ğ‘–=0
ğ‘›âˆ’1
Resize
{ğ’›ğ‘¡âˆ’1
ğ‘– }ğ‘–=0
ğ‘›âˆ’1 ğ’›ğ‘¡âˆ’1
ğ‘ ğ’›ğ‘¡âˆ’1
ğ‘
Dynamic 
Integration
Latent Space 
Smoothness
Regional 
Enhancement
{à·œğ’›ğ‘¡âˆ’1
ğ‘– }ğ‘–=0
ğ‘›âˆ’1
ğŸâˆ’ğ
ğ
Multi-agent 
Collaboration based 
Scene Parsing
ğ’›ğ‘¡âˆ’1
Conductor
Object 
Extraction Agent
 Evaluator
Input
Prompt
Prompt
Set
â€œrefrigeratorâ€: â€œ[0.76, 0.20, 0.18, 0.34, 2]",
 â€œwindowâ€: â€œ[0.01, 0.05, 0.47, 0.47, 4]",
â€œplantâ€: â€œ[0.05, 0.21, 0.18, 0.31, 1]",
â€œtableâ€: â€œ[0.18, 0.48, 0.64, 0.41, 0]",
â€œrugâ€: â€œ[0.15, 0.81, 0.80, 0.17, 3]" }
Layout Agent
MCP HCD
Multi-agent system
Action Relations 
Extraction Agent
Background 
Extraction Agent
Spatial Relations 
Extraction Agent Layout Agent
 Aesthetics 
Enhancement agent
â€¦â€¦
â€œrefrigerator": "A silver refrigerator",
 "window": "a large window",
"plant": "a lush green plant",
"table": "a wooden table",
"rug": "a rug"
Object Extraction Agent
" refrigerator": "A sleek, black refrigerator ....",
"window": "The large window spans across ... ",
"plant": "A lush green plant sits in a simple ... ",
"table": "The wooden dining table stands ... ",
"rug": "Beneath the table, a soft, woven rug ... "
Aesthetic Enhancement agent
CA CA CA CA
ğ’›ğ‘¡
â€¦
ğ’›ğ‘¡âˆ’1
â€¦
ğ’›0
Generated Image
Denoising U-Net
â†‘
â†‘
â†‘
â†“
â†“
The dining area exudes calm 
and warmth, with light-
colored walls and hardwood 
floors that complement the 
room's natural tones.
Background Extraction Agent
cross-attention
plant
table
rug
Figure 1. The overall framework of the proposed MCCD. MCCD consists of two core components: Multi-agent Collaboration-based
scene Parsing (MCP) module and Hierarchical Compositional Diffusion (HCD) module. In MCP, the blue and green arrows indicate
forward CoT reasoning and backward feedback processes, respectively
3.2. Multi-agent Collaboration based Scene Parsing
Based on the sufficient consideration of the composition of
complex scenes, we categorize the scene elements into sev-
eral parts: objects and their characteristics, backgrounds,
action relations, spatial relations, and layouts. To generate
complex scenes, we need to explore all the key elements
in the text prompt thoroughly. The previous method [39]
utilizes the Chain-of-Thought (CoT) capability of MLLMs
to extract objects and layouts from the input text prompt
to a certain extent. Despite some success, when dealing
with more complex scenarios, the CoT-based approach of-
ten fails to sufficiently parse the intricate relations in the
scene. Furthermore, unidirectional CoT reasoning lacks er-
ror correction mechanisms, leading to inaccurate elemen-
tal parsing and image generation. Therefore, we propose
a Multi-agent Collaboration-based Scene Parsing (MCP)
module that splits the scene parsing process into multiple
sub-stages and constructs corresponding specialized agents
for each stage, and generates a multi-agent system simulta-
neously. In the system, each agent dynamically collaborates
and interacts with other agents while strictly performing its
specialized tasks, thus maximizing the advantages of team-
work. This paradigm ensures that MLLMs accurately parse
multiple elements contained in text prompts, providing sig-
nificant advantages over traditional CoT reasoning.
The constructed multi-agent system consists of six
agents whose definitions and tasks are: (1) object extraction
agent, whose task is to extract objects and their character-
istics from text prompts. (2) Background extraction agent,
whose task is to extract the object-independent background
descriptions. (3) Action relations extraction agent, whose
task is to capture the action relations between the objects.
(4) Spatial relations extraction agent, whose task is to cap-
ture the spatial relations between the objects. (5) Layout
agent, whose task is to conceptualize the layout of objects
for a reasonable composition. (6) Aesthetics enhancement
agent, whose task is to beautify the objectsâ€™ characteristics
to enhance the image aesthetics. Our goal is to integrate the
outputs of multiple agents to ultimately generate the objects
and characteristics prompt, background prompt, and bound-
ing boxes of all objects. Each agent is defined by an MLLM
and is associated with a prompt template while allowing ac-
cess to an external knowledge base.
Moreover, we construct a conductor C and an evalua-
tor E to coordinate multiple agents for effective collabora-
tion. Specifically, the conductor dynamically directs dif-
ferent agents to construct a forward CoT in iterations and
passes candidate answers to the external program execution
environment. The evaluator then uses feedback signals to
trigger a backward feedback process. These two processes
are elaborated below.
Forward CoT Reasoning. In forward CoT reasoning,
the conductor C adaptively determines the entire set of
participating agents, casting the procedure as a sequen-
tial decision-making problem in which each possible ac-
tion corresponds to a particular agent selection. We de-
fine the input prompt as P and a set of predefined agents
as Î¾ = {AÏ•1 , AÏ•2 , . . . ,AÏ•n }, where n is the total num-
ber of agents and Ï•i is the configuration of the agent i-
th. The set of output for the t-th inference step is denoted
3
as Ot, and the state is denoted as St = (P, Ot, t). With
the powerful prompt learning capability of MLLM, agents
can achieve the same functionality in a training-free man-
ner as decision-making agents that require large amounts of
data for training in traditional reinforcement learning, with
significant advantages in terms of efficiency and overhead.
Thus, we take the conductor as a policy function for select-
ing an agent, denoted as:
C(a | s) =Pr {AÏ•t = a | St = s}. (1)
The agent selection strategy can be translated into the de-
sign of the prompt template, which requires prompt engi-
neering to realize the optimal strategy. Each step of forward
CoT reasoning is denoted as:
AÏ•it = C (St) , (2)
o = AÏ•it (P, Ot) , (3)
Ot+1 = Ot âˆª {o}, (4)
where AÏ•it represents the selected it-th agent at step t and
o denotes the output of the selected agent. After reaching
the maximum step T, the forward process is aborted and all
outputs are integrated into the result R.
Backward feedback.The backward feedback strategy en-
ables a multi-agent system to adjust collaborative behav-
ior by using the evaluatorâ€™s evaluation of the forward re-
sponse. The execution order of the agents is defined as
Î· = {AÏ•i1 , AÏ•i2 , . . . ,AÏ•in }, where it denotes the index
of the agent at step t. The backward feedback process be-
gins with an external feedback fraw, usually provided by
the program execution environment, denoted as
fraw = execution(R). (5)
The raw feedback is then evaluated by evaluatorE to obtain
an initial signal: (f0, sf0) = E(fraw), where f0 indicates
whether the backward process needs to continue or not, and
sf0 indicates the localization of the error in the backward
feedback. If the result R in the forward reasoning is cor-
rect, then f0 is set to false and the whole reasoning process
ends. Otherwise, the conductor C initiates a backward feed-
back process that updates the response by back-propagating
from the last agent. At step t backward, the state update is
represented as follows:
(ft, sft) â† feedback

AÏ•iTâˆ’t+1
, P, Ot, ftâˆ’1

, (6)
Ot+1 = Ot âˆª {sft}. (7)
The backward feedback process continues to be executed
iteratively until the feedback signal indicates that the agent
has made a mistake or until all the agents have undergone
reflection. Once this occurs, the forward process will then
be executed again. This process is repeated continuously
until a reasonable answer is produced.
3.3. Hierarchical Compositional Diffusion
Previous region-based diffusion methods, such as [39], di-
vide images into multiple complementary regions. It takes
into account spatial relations to a certain extent, but it
cannot cope with more complex scenes due to the non-
overlapping and independent properties of regions. Addi-
tionally, [35] imposes constraints on cross-attention maps
so that the positions and sizes of objects are as consis-
tent as possible with the bounding box, but ignores the
consideration of the overlaps between the bounding boxes
and smoothness near the bounding box, leading to unre-
alistic scene synthesis. To this end, we propose a Hier-
archical Compositional Diffusion (HCD) module that per-
forms progressive interaction of multiple elements obtained
from scene parsing. Specifically, we dynamically balance
the overlapping regions between multiple objects using the
Gaussian mask and a regional enhancement strategy is used
to enlarge the discrepancy between objects and background
in the latent space. Moreover, Gaussian smoothing is used
to enhance the smoothness around the bounding box.
As shown in Figure 1, the MCP parses a complex prompt
with n objects into multiple prompts,i.e., the input complex
prompt pc, the object prompts {pi
o}nâˆ’1
i=0 and background
prompt pb. At each timestep, we feed each prompt into the
denoising network in parallel, using cross-attention layers
to generate the corresponding latent representations.
ztâˆ’1 = Softmax
(WQ Â· Ï• (zt) (WK Â· Ïˆ (p))âˆš
d

(WV Â· Ïˆ (p)) ,
(8)
where image latent zt is the query and each prompt is
the key and the value. WQ,WK,WV are linear projec-
tions and d is the latent projection dimension of the keys
and queries. The latent representations of complex prompt,
all object prompts, and background prompt are denoted as
zc
tâˆ’1, {zi
tâˆ’1}nâˆ’1
i=0 , and zb
tâˆ’1. We use bilinear interpolation
to resize the latent representation of the object based on the
dimensions of the bounding box, denoted as follows:
Ë†zi
tâˆ’1 = R
 
zi
tâˆ’1, Bi
. (9)
Complex scenes often contain multiple objects with in-
tricate positions and action relationships across multiple ob-
jects, resulting in a large number of overlapping regions.
To this end, we design a dynamic integration mechanism
based on a depth-aware Gaussian mask to achieve adaptive
and smooth feature fusion in overlapping regions. In gen-
eral, the center region of the bounding box contains the core
features of the object, while the features in the edge region
gradually decrease in importance. To keep as many core
features as possible in the overlapping regions of the bound-
ing box, for each bounding box (x0, y0, w, h), we construct
a Gaussian mask matrix that contains weights that gradually
4
SD1.5SD1.5 + Ours
â€œThe bright orange pumpkinsat next tothe crisp green apples.â€
SD2.0-baseSD2.0-base + OursSD2.0SD2.0 + OursSDXL-baseSDXL-base + Ours
â€œThe greenplant was on the right of the whitewindow.â€
â€œThe warm yellow lightshone downon the colorfulflower bouquet.â€
â€œThe soft pink petals of the flower contrasted with the rough grey sidewalk.â€
â€œThe smooth, glossy finish of the ceramic vase accentuated the vibrant colors of the flowers, a stunning centerpiece of beauty.â€
SD1.5SD1.5 + OursSD2.0-baseSD2.0-base + OursSD2.0SD2.0 + OursSDXL-baseSDXL-base + Ours
SD1.5SD1.5 + OursSD2.0-baseSD2.0-base + OursSD2.0SD2.0 + OursSDXL-baseSDXL-base + Ours
SD1.5SD1.5 + OursSD2.0-baseSD2.0-base + OursSD2.0SD2.0 + OursSDXL-baseSDXL-base + Ours
SD1.5SD1.5 + OursSD2.0-baseSD2.0-base + OursSD2.0SD2.0 + OursSDXL-baseSDXL-base + Ours
Figure 2. Qualitative results of MCCD improving diffusion models. MCCD enhances the attribute binding and spatial relationships of
the base diffusion models. The generated results have reasonable backgrounds and detailed textures with great aesthetics and realism.
decay from the center to the edges, represented as follows:
M(x, y) = exp
 
âˆ’(x âˆ’ Âµx)2 + (y âˆ’ Âµy)2
2Ïƒ2
!
, (10)
where (Âµx, Âµy) = (x0 + w/2, y0 + h/2) is the center of
the mask matrix, and the Ïƒ = max(w, h)/2 is the standard
deviation controlling the width of the Gaussian distribution.
Furthermore, an objectâ€™s layer depth d indicates its front
and back positions in the image. Objects with smaller layer
depth appear closer in the image, so greater weights should
be assigned during feature fusion. To achieve a smooth
transition between foreground and background, we calcu-
late continuous and eased layer depths:
wi = 1
1 + exp
 
Î± Â·
 
di âˆ’ nâˆ’1
2
, (11)
where d âˆˆ {0, 1, . . . , nâˆ’1} is the layer depth of the object,
and Î± is the smoothness control parameter. Given any co-
ordinates (x, y) of the overlapping regions of the bounding
boxes, their dynamically fused features are represented as.
Ë†zi
tâˆ’1(x, y) =
Pm
i=1 wi Â· Mi(x, y) Â· zi
tâˆ’1(x, y)Pm
i=1 wi Â· Mi(x, y) , (12)
where m is the number of overlapping regions. Then, we
concatenate the latent representations of all object prompts
5
Table 1. Evaluation results on T2I-CompBench. MCCD shows the best performance in terms of Attribute Binding, Object Relationship,
and Complex. Basic data is derived from [15].
Models Attribute Binding Object Relationship Complexâ†‘
Colorâ†‘ Shapeâ†‘ Textureâ†‘ Spatialâ†‘ Non-Spatialâ†‘
Composable Diffusion [23] 0.4063 0.3299 0.3645 0.0800 0.2980 0.2898
Structured Diffusion [10] 0.4990 0.4218 0.4900 0.1386 0.3111 0.3355
Attn-Exct v2 [3] 0.6400 0.4517 0.5963 0.1455 0.3109 0.3401
GORS [15] 0.6603 0.4785 0.6287 0.1815 0.3193 0.3328
DALL-E 2 [29] 0.5750 0.5464 0.6374 0.1283 0.3043 0.3696
PixArt-Î± [4] 0.6886 0.5582 0.7044 0.2082 0.3179 0.4117
SD1.5 [30] 0.3134 0.2954 0.3772 0.1087 0.3015 0.2994
SD1.5 +MCCD 0.3508 0.3193 0.4026 0.1462 0.3078 0.3054
SD2.0-base [30] 0.4498 0.3584 0.4319 0.1140 0.3045 0.3021
SD2.0-base +MCCD 0.4823 0.3702 0.4621 0.1613 0.3106 0.3146
SD2.0 [30] 0.4852 0.4066 0.4591 0.1490 0.2905 0.3173
SD2.0 +MCCD 0.5090 0.4170 0.4921 0.1785 0.3123 0.3250
SDXL-base [2] 0.5744 0.4705 0.4907 0.1971 0.3009 0.3130
SDXL-base +MCCD 0.6278 0.4832 0.5647 0.2350 0.3132 0.3348
based on the positions of the bounding boxes to achieve
control over positional relations. The area not covered by
the bounding box is filled by the latent representation of the
background. This process is represented as:
zâ€²
tâˆ’1 = Concat({MBi Â· Ë†zi
tâˆ’1}nâˆ’1
i=0 , {MÂ¬(
Snâˆ’1
i=0 Bi) Â· zb
tâˆ’1}),
(13)
where MBi and MÂ¬(
Snâˆ’1
i=0 Bi) are masks denoting the re-
gion within the bounding boxBi and the background region
outside all bounding boxes, respectively.
To ensure that objects are generated within the desig-
nated bounding box regions, we implement a regional en-
hancement mechanism that emphasizes the latent represen-
tation of the bounding box regions and simultaneously sup-
presses the influence of surrounding background areas.
zâ€²
tâˆ’1+ =Î»pos Â· MBi Â·
 
Max
 
Ë†zi
tâˆ’1

âˆ’ zâ€²
tâˆ’1

, (14)
zâ€²
tâˆ’1âˆ’ = Î»neg Â·

MÂ¬(
Snâˆ’1
i=0 Bi)

Â·
 
zâ€²
tâˆ’1 âˆ’ Min
 
Ë†zi
tâˆ’1

,
(15)
where Î»pos and Î»neg are both set to 0.2.
The features inside and outside the bounding box have
large discrepancies, resulting in an unsmooth excess of the
generated image near the bounding box. Therefore, we
employ Gaussian filtering to smooth the features near the
bounding box in the latent space. First, we define a two-
dimensional Gaussian kernel, denoted as follows:
GÏƒ(i, j) = 1
2Ï€Ïƒ2 exp

âˆ’i2 + j2
2Ïƒ2

, (16)
where Ïƒ = 1.0 is the standard deviation that controls the
spread of the Gaussian filtering. Then, assuming an arbi-
trary feature with coordinates (x, y) near the bounding box,
we perform Gaussian filtering on it, denoted as follows:
zsmooth
tâˆ’1 (x, y) =
kX
i=âˆ’k
kX
j=âˆ’k
zâ€²
tâˆ’1(x + i, y+ j) Â· GÏƒ(i, j).
(17)
Furthermore, to preserve the overall semantics and distri-
bution of the image as much as possible, the weighted sum
of zc
tâˆ’1 and zsmooth
tâˆ’1 is used to produce the final denoised
output that maintains consistency between image and text.
ztâˆ’1 = Âµ Â· zc
tâˆ’1 + (1âˆ’ Âµ) Â· zsmooth
tâˆ’1 , (18)
where Âµ is the trade-off weight set to 0.8.
6
â€œA man in a black jacketstandingin a kitchen next to a graydog.â€
w/o Complexpromptw/o RegionalEnhancementw/o DynamicIntegrationw/o Smoothness
â€œThe soft, furry kittens played together in a pile on the warm, cozy blanket, their tiny paws batting at colorful balls of yarn.â€
ours
w/o MCP
Figure 3. Ablation results of MCCD. The poor results after removing the critical components prove that each component is crucial.
â€œThe soft, warm glowof the campfireilluminated the faces of the hikers, as they roasted marshmallows and swapped stories. Astarry sky stretches over a distant mountainrange.Nearby, a tent suggests that the group is on a camping adventure.â€
â€œThe dining area is filled with natural light. A silver refrigerator stands nearby, while a large window brightens the space and frames a lush green plant sitting just in front of it. The dining area features a wooden table, and a rug adds warmth to the room, creating a cozy atmosphere.â€
Figure 4. Additional qualitative results. MCCD can handle complex text prompts with multiple objects and attribute binding relationships
effectively, generating reasonable bounding box layouts and producing aesthetically pleasing images with high realism.
4. Experiments
4.1. Datasets and Evaluation Metrics
Since our framework is training-free, no dataset is required
for training. To comprehensively evaluate the performance
of different methods, we selected six metrics from three
categories in the T2I-CompBench benchmark [15], namely
Attribute Binding (Color, Shape, Texture), Object Rela-
tionship (Spatial, Non-Spatial), and Complex. The three
metrics of Attribute Binding are evaluated by disentangled
BLIP-VQA. The spatial metric is determined by detecting
the object through UniDet and comparing the center of the
objectâ€™s bounding box in the image to determine the spa-
tial relationship. CLIP-Score is applied for the evaluation
7
of the Non-Spatial metric. The Complex metric is the av-
erage score of disentangled BLIP-VQA, CLIP-Score, and
UniDet. Each metric corresponds to 300 prompts, and each
prompt yields 10 images for evaluation with diverse seeds.
4.2. Implementation Details
Our MCCD is both generic and extensible, allowing us to
seamlessly integrate a wide range of MLLM architectures
and diffusion models into the framework. In our experi-
ments, we use GPT-4o-mini [1] to construct multiple agents
in MCP. The pre-trained diffusion models in HCD consist
of: SDv1.5 [30], SDv2 [30], SDv2-base [30], and SDXL-
base [26]. The inference process is set to 20 steps, with the
classifier-free guidance scale set to 7.0. We have carefully
designed task templates and selected high-quality context
examples to ensure optimal performance. All experiments
are conducted on a single A800 GPU.
4.3. Main Results
Qualitative Results. Figure 2 shows the results of MCCD
on multiple diffusion models. We have the following obser-
vations (i) Attribute Binding: Attributes ( e.g., color, quan-
tity) are incorrectly bound to objects in the generated re-
sults of the base diffusion models, while each attribute is
correctly bound to the corresponding object after applying
MCCD. (ii) Spatial location relationships are often incor-
rect in the results generated from the base diffusion models.
For example, objects may be incorrectly placed in unnatu-
ral locations, or spatial relationships between objects may
be confused. The spatial relationship confusion problem is
solved after applying MCCD. (iii) Realism and aesthetics:
Many of the generated results of the base diffusion models
lack suitable backgrounds and detailed texture structures,
while MCP provides detailed, realistic, and aesthetically
pleasing descriptions of objects and backgrounds for the
complex prompts, which makes the results highly aesthetic.
Figure 4 shows the results of complex prompts with
multiple objects and attribute binding by MCCD. Through
in-depth understanding and parsing of the input complex
prompts, MCCD can effectively recognize and extract the
objects and their interrelationships in the text prompt and
then generate a reasonable and accurate bounding-box lay-
out. The position and size of each bounding box closely
correspond to the elements (e.g., objects, colors, shapes,
etc.) in the complex prompt, ensuring the accurate posi-
tioning and realistic rendering of each element in the im-
age. In addition, MCCD fully captures the connection be-
tween the object and its surroundings, resulting in an im-
age that exhibits a high degree of realism and aesthetics. In
this way, MCCD can transform complex and layered text
prompts into clearly structured and highly realistic images,
perfectly combining text and visual expression.
Quantitative Results. Table 1 shows the results of mul-
tiple open source reproducible models and MCCD in T2I-
CompBench. We can draw the following conclusions: (i)
Overall, the best results are achieved by applying MCCD
to SDXL, with an overall metric improvement of 9.04%,
which is significantly better than the baseline models. (ii)
Compared to other diffusion models, the application of
MCCD resulted in the highest overall improvement in met-
rics of 9.04%, demonstrating the general scalability of
MCCD. (iii) Benefiting from MCP, the key components of
MCCD, the Spatial metric of the diffusion model is in-
creased by up to 41.49%, and the Complex metric is in-
creased by up to 8.73%, which effectively improves the
spatial position relationship, semantic expressiveness, and
image fidelity of the generated images.
4.4. Ablation Study
To verify the necessity of each component, we conduct full-
scale ablation studies. The results are shown in Figure 3. (i)
Firstly, MCP is removed from MCCD. The poor attribute
binding and spatial location relationship indicate that it is
crucial to dynamically invoke multiple agents to generate
reasonable bounding boxes, objects, and background de-
scriptions before image generation. (ii) In addition, we re-
move the complex prompt in HCD, and the image gener-
ation results that are inconsistent with the prompt descrip-
tions suggest that the complex prompt is indispensable for
maintaining consistency between image and text. (iii) Then,
Regional Enhancement is removed from HCD. The phe-
nomenon of poor semantic representation of the objects and
the background in the image suggests that regional enhance-
ment is crucial. (iv) Additionally, we remove Dynamic In-
tegration, and the image appears to be partially missing in
the occluded objectâ€™s part, presenting an anomalous struc-
ture. (v) Finally, we remove Latent Space Smoothness, and
the image exhibits unnatural transitions where the bounding
box connects to the background, indicating that smoothness
is essential for natural transitions between the bounding
boxes and between the bounding box and the background
in the image.
5. Conclusion
In this paper, we propose Multi-Agent Collaboration-based
Compositional Diffusion (MCCD) for generating high-
quality complex scenes. Specifically, we design a multi-
agent collaboration-based scene parsing module to fully ex-
tract the scene elements contained in the text by construct-
ing a multi-intelligentsia system based on MLLMs. In ad-
dition, we propose a hierarchical compositional diffusion
that utilizes dynamic integration of overlapping regions, re-
gional enhancement, and latent space smoothness to gener-
ate realistic and aesthetically pleasing images. Comprehen-
sive experiments prove the advantages of our method.
8
MCCD: Multi-Agent Collaboration-based Compositional Diffusion for
Complex Text-to-Image Generation
Supplementary Material
6. The Prompt design of MCP
We provide detailed descriptions and prompt template im-
plementations for the conductor, evaluator, and all agents.
The text enclosed within the curly braces denotes place-
holders that will be dynamically populated during runtime
based on the input text prompt and the agentâ€™s output.
6.1. Conductor
The role of a conductor is highly specialized and signif-
icant, which necessitates a more intricate prompt design
compared to other agents. The task of the conductor is to
coordinate all the agents you manage so that they can work
together to solve the problem. The prompt template for a
conductor is shown in Table 2.
6.2. Evaluator
The evaluatorâ€™s task is to exercise critical thinking to assess
the truthfulness and reasonableness of the agentâ€™s output. If
it is not reasonable, then make recommendations for modi-
fication. The prompt template for the evaluator is given in
Table 3.
6.3. Agent System
In this section, we provide an in-depth overview of the in-
dividual agents involved in our MCCD. Each agent is as-
signed a specific role and domain knowledge related to
problem-solving.
Object extraction agent.The object extraction agentâ€™s task
is to extract key entities and their corresponding character-
istics from the text input prompt. The prompt template for
the object extraction agent is shown in Table 4.
Background extraction agent.The task of the background
extraction agent is to extract the background from this com-
plex prompt. The extracted background is required not to
contain any object and its characteristics, but only a descrip-
tion of the scene as a whole. The prompt template for the
background extraction agent is shown in Table 5.
Action relations extraction agent.The task of the action
relation extraction agent is to extract the action relations be-
tween objects in the scene, such as holding, sitting, and so
on, to bind multiple objects. Its prompt template is illus-
trated in Table 6.
Spatial relations extraction agent.The task of the spatial
relations extraction agent is to extract the spatial relations
between objects in the scene, such as left, beside, and so
on, to fully extract the spatial information of the scene. Its
prompt template is illustrated in Table 7.
Table 2. The prompt template for the conductor.
Conductor
[INST]<SYS>
You are the leader of an agent system for text parsing in
complex scenes. Now, you need to coordinate all the
agents you manage so that they can work together to
solve the problem. Next, you are given a specific text
prompt, and your goal is to select the agents you think
are best suited to solicit insights and suggestions. Gen-
erally speaking, the parsing of complex scenes includes
several processes: object extraction, background extrac-
tion, relation extraction, layout extraction, and aesthetic
optimization. Different text prompts may correspond to
different processes, so you need to select the correspond-
ing agent to solve the problem dynamically. </SYS>
<USER>
The text prompt is: {text prompt}.
Remember, based on the capabilities of different agents
and the current status of the problem-solving process,
you need to decide which agent to consult next. The
agentsâ€™ capabilities are described as follows: {agent
info}.
Agents that have already outputted their answers include:
{outputted agents} .
Please select an agent to consult from the remaining
agents {remaining agents}.
Remember, the agent must choose from the existing list
above.
Note that you must complete the workflow within the
remaining {remaining steps} steps.
You should output the name of the agent directly. The
next agent is: </USER>[INST]
Layout agent.The task of the layout agent is to layout the
scene by generating a bounding box for each object. Its
prompt template is illustrated in Table 8.
Aesthetics enhancement agent.The task of the aesthetic
enhancement agent is to perform the role of an aesthetic
guide to optimize the description of the object characteris-
tics, thus enhancing the artistic and aesthetic qualities of the
image. Its prompt template is displayed in Table 9.
7. Case Study of MCP
Figures 5 and 6 show two cases of MCP workflows. As
shown in the figures, the conductor organizes multiple
9
Table 3. The prompt template for the evaluator.
Evaluator
[INST]<SYS>
Your task is to exercise critical thinking to assess the
truthfulness and reasonableness of the agentâ€™s output.
If it is not reasonable, then make recommendations for
modification.
Output format: {â€œResultâ€:â€œEvaluation results, with a
value of right or wrongâ€, â€œProblemâ€: â€œIf there is a prob-
lem, describe it in detail, otherwise, the value is nullâ€,
â€œModification Suggestionâ€: â€œIf the result is incorrect,
describe the proposed change, otherwise, the value is
nullâ€}. </SYS>
<USER>
The input prompt is described as {input prompt}.
The output of all agents is {outputs}.
Please evaluate the reasonableness of the agentsâ€™ outputs.
If they are not reasonable, please state your suggestions
for modification.
</USER>[INST]
Table 4. The prompt template for the object extraction agent.
Object extraction agent
[INST]<SYS>
As an object extraction agent, you extract key entities
and their corresponding characteristics from the text in-
put prompt.
Extract multiple object and characteristic pairs if multi-
ple characteristics of an entity describe different parts of
a person, such as the head, clothes/body, and underwear.
To ensure numeric accuracy, objects with the same class
name ( e.g., five apples) will be separately assigned to
different regions.
The output format is {object1:characteristic1,
object2:characteristic2, . . . , objectn:characteristicn}
</SYS>
<USER>
The input prompt is described as: {input prompt}.
You are supposed to refer to the output of other agents:
{outputs}.
Please output the extracted objects and their characteris-
tics in the text prompt.
</USER>[INST]
agents in an orderly manner according to the input prompt
to achieve a well-collaborated result. Figure 5 is a case
where each agent correctly outputs the result. When a prob-
lem occurs during the systemâ€™s execution of a task, the Eval-
Table 5. The prompt template for the background extraction agent.
Background extraction agent
[INST]<SYS>
As an object extraction agent, your task is to extract the
background from this complex prompt. The extracted
background is required not to contain any object and its
characteristics, but only a description of the scene as a
whole.
</SYS>
<USER>
The input prompt is described as: {input prompt}.
The outputs given by other agents are as follows:
{outputs}, please refer to them carefully.
Please extract and output the background in the text
prompt.
</USER>[INST]
Table 6. The prompt template for the action relations extraction
agent.
Action relation extraction agent
[INST]<SYS>
As an action relation extraction agent, your task is to ex-
tract the action relations between objects in the scene,
such as holding, sitting, and so on, to fully extract the
spatial information of the scene.
The output format is{(object1, action relation1, object2),
. . . , ((object2, action relation2, objectn)}
</SYS>
<USER>
The input prompt is described as: {input prompt}.
The outputs given by other agents are as follows:
{outputs}.
Please extract and output the action relations between
objects in the text prompt.
</USER>[INST]
uator can identify it through a backward feedback process
and suggest modifications to resolve it quickly. For exam-
ple, in the case of Figure 6, when the behavior of a certain
agent does not meet the expectation, the Evaluator analyzes
that the problem occurs in the layout agent and suggests a
modification. The backward feedback process identifies the
agentâ€™s error and regenerates the prompt set. This mecha-
nism of feedback and correction provides a high degree of
flexibility and self-adaptation for the system, thus enhanc-
ing the robustness and intelligence of the whole system.
10
Table 7. The prompt template for the spatial relations extraction
agent.
Spatial relation extraction agent
[INST]<SYS>
As a spatial relation extraction agent, your task is to ex-
tract the spatial relations between objects in the scene,
such as â€œleftâ€ and â€œbesideâ€, to fully capture the spatial
information of the scene.
The output format is {(object1, spatial relation 1,
object2), . . . , (object2, spatial relation2, objectn)}
</SYS>
<USER>
The input prompt is described as: {input prompt}.
You should refer to the output of other agents:
{outputs}.
Please extract and output the spatial relations between
objects in the text prompt.
</USER>[INST]
Table 8. The prompt template for the layout agent.
Layout agent
[INST]<SYS>
You are a layout agent, and your task is to generate the
bounding boxes for the objects. The following rules must
be strictly followed during generation. A layout denotes
a set of â€œobject: bounding boxâ€ items. â€œobjectâ€ means
any object name, which starts the object name with â€œaâ€
or â€œanâ€ if possible. â€œbounding boxâ€ is formulated as
[x, y, w, h, d], where â€œx, yâ€ denotes the top left coor-
dinate of the bounding box, â€œwâ€ denotes the width, â€œhâ€
denotes the height, and â€œdâ€ indicates the order of the ob-
jectâ€™s front and back position in the image, starting from
0, the smaller d indicates the object is more forward. The
top-left corner has coordinates [0, 0]. The bottom-right
corner has coordinates [1, 1].
The output format: â€œ {object1: bounding box 1, object2:
bounding box2, . . ., objectn: bounding boxn}â€
</SYS>
<USER>
The input prompt is described as: {input prompt}.
You should refer to the output of other agents:
{outputs}.
Please generate and layout according to the task descrip-
tion and rules.
</USER>[INST]
Table 9. The prompt template for the layout agent.
Aesthetics enhancement agent
[INST]<SYS>
As an aesthetic enhancement agent, you serve as an aes-
thetic guide, refining the descriptions of an objectâ€™s char-
acteristics to amplify its artistic and aesthetic appeal.
This involves thoughtful consideration of composition,
color balance, texture, and other key elements contribut-
ing to the imageâ€™s overall impact.
</SYS>
<USER>
The input prompt is: {input prompt}.
You should refer to the output of other agents:
{outputs}.
Please generate a glorified characterization as required.
</USER>[INST]
8. Additional Ablation Results of MCP
To demonstrate the scalability and generalizability of
MCCD, we perform comprehensive ablation experiments
on MLLMs used in MCP based on the SDXL-Base model.
We select GPT-4o-mini [1], GPT-4o [1], and LLaVa-1.5-7b
[21] as the MLLMs. The qualitative results are shown in
Figure 7. According to the experimental results, compared
to the errors in object attribute binding and quantity genera-
tion of the SDXL-Base model, MCCD can generate correct
image content with high fidelity and aesthetic quality af-
ter utilizing different MLLMs. Specifically, these models
can accurately recognize and bind the attributes of objects
while exhibiting high stability and consistency in the num-
ber and layout of objects, which significantly improves the
quality and usability of the generated results. This suggests
that MCCD can fully utilize the potential of these advanced
MLLMs to generate images with high visual realism and
artistic value in complex scenes.
9. Additional Qualitative Analysis
Figure 8 shows the results of complex text prompts with
multiple objects and attribute bindings by MCCD. Through
an in-depth understanding and parsing of the input detailed
text description, MCCD can effectively recognize and ex-
tract the objects and their interrelationships in the text, gen-
erating a reasonable and accurate bounding box layout. The
position and size of each bounding box closely correspond
to the elements (e.g., objects, colors, shapes, etc.) in the tex-
tual descriptions, ensuring the accurate positioning and re-
alistic rendering of each element in the image. In addition,
MCCD not only focuses on the physical positional relation-
ship of the object but also fully captures the interaction be-
11
Conductor
EvaluatorPromptSet
Object Extraction Agent
Action Relations Extraction Agent
Background Extraction Agent
Spatial Relations Extraction Agent
Layout Agent
Aesthetics Enhancement Agent
â€œabartenderâ€:â€œAbartenderstandsatthebarcounter,skillfullypreparingthenextdrinkâ€,â€œcocktaildrinksâ€:â€œAsetofcocktaildrinksarrangedinalineâ€,â€œabarcounterâ€:â€œabarcounterlinedwithcocktaildrinks,partofalivelyatmosphereâ€
Object  Extraction Agent
A cozy bar atmosphere with wooden tones and soft lighting.
Background Extraction Agent
(â€œa bartenderâ€, â€œnext toâ€, â€œa bar counterâ€), (â€œcocktail drinksâ€, â€œon top ofâ€, â€œa bar counterâ€)
Spatial Relations Extraction AgentAction Relations Extraction Agent
(â€œa bartenderâ€, â€œstands next toâ€, â€œa bar counterâ€), (â€œa bartenderâ€, â€œpreparesâ€, â€œcocktail drinksâ€), 
â€œa bartenderâ€: â€œ[0.10, 0.39, 0.29, 0.59, 0]â€, â€œcocktail drinksâ€: â€œ[0.44, 0.44, 0.29, 0.29, 1]â€, â€œa bar counterâ€: â€œ[0.00, 0.44, 1.00, 0.44, 2]â€
Layout AgentAesthetics Enhancement agentâ€œa bartenderâ€: â€œA bartender wears a neatly pressed uniform, and the warm lighting casts a gentle glow on his face, highlighting his focus and expertise.â€,â€œcocktail drinksâ€: â€œA set of cocktail drinks arranged in a line, shimmering under the ambient lighting. The drinks feature a variety of colors, from deep reds to bright oranges, each garnished with a twist of citrus or a sprig of mint, adding an artistic touch.â€,â€œa bar counterâ€: â€œThe bar counter is sleek and polished, its dark wood surface reflecting the soft glow of overhead lights. The counter is neatly organized, creating a sense of order and casual elegance atmosphere.â€
Evaluatorâ€™sFeedbackâ€œResultâ€: â€œrightâ€,â€œProblemâ€: â€œâ€,â€œModification Suggestionâ€: â€œâ€PromptSet
Inputprompt:â€œA set of cocktail drinksare arranged in a line on top of abar counter. The bartenderstands next tothe bar counter,  skillfully preparing the next cocktail drink, adding to the lively atmosphere.â€
â‘ 
â‘¡
â‘¢
â‘£
â‘¤
â‘¥
â‘ 
â‘¡
â‘¢
â‘£
â‘¤
â‘¥â‘¦ â‘¦
Start
ForwardCoTReasoning
Figure 5. A case to illustrate the workflow of MCP.
tween the object and its surroundings, resulting in an image
that exhibits a high degree of realism and aesthetics. In this
way, MCCD can transform complex and layered textual in-
formation into clearly structured and visually rich images,
perfectly combining text and visual expression. MCCD can
generate reasonable bounding box layouts, resulting in at-
tractive images with high realism.
10. Discussion of Broader Impact
MCCD, as a training-free T2I approach with excellent per-
formance, is capable of transforming complex and layered
text information into clearly structured and visually rich im-
ages, perfectly combining text and visual representation.
However, it also has potential negative impacts. For exam-
ple, it may be used to generate scenarios involving immoral
or illegal activities that can harm society. Additionally, au-
tomatically generated images may raise concerns about in-
tellectual property and copyright.
11. Discussion of Limitations and Future Work
The proposed MCCD serves as a training-free plug-in that
can be adapted to any Diffusion-based T2I methods, using a
hierarchical compositional generative paradigm to enhance
the quality of complex scene generation for the model. A
potential problem is that the MCCD inference overhead is
somewhat affected by the number of objects in a complex
text prompt. As the number of bounding boxes increases,
the inference time also increases. In the future, we will fo-
cus on designing efficient performance optimization strate-
gies to improve the inference speed of the method.
Acknowledgements
This work is supported in part by the Shanghai Munici-
pal Science and Technology Committee of Shanghai Out-
standing Academic Leaders Plan (No. 21XD1430300), and
in part by the National Key R&D Program of China
(No. 2021ZD0113503).
References
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4 technical report. arXiv preprint arXiv:2303.08774 ,
2023. 8, 11
[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce
12
Conductor
Evaluator
Prompt
Set
Object 
Extraction Agent
Background 
Extraction Agent
Spatial Relations 
Extraction Agent
Layout Agent
Aesthetics 
Enhancement 
Agent
â€œwindowâ€: â€œlarge, fills the room, offers a view of the peaceful outdoorsâ€,
  â€œbedâ€: â€œcomfortable, is placed at the center, dressed in crisp white linensâ€,
  â€œnightstandâ€: â€œsleek wooden, placed to the left of the bedâ€,
  â€œlampâ€: â€œsmall, on top of the nightstand, casting a warm glowâ€,
  â€œrugâ€: â€œplush, adds texture and warmth, placed on the floorâ€
Object  Extraction Agent
The room offers a calm, inviting atmosphere, 
bathed in soft natural light with a peaceful, 
serene vibe.
Background Extraction Agent
(â€œnightstandâ€, â€œto the left ofâ€, â€œbedâ€),
  (â€œlampâ€, â€œon top ofâ€, â€œnightstandâ€),
Spatial Relations Extraction Agent
â€œa windowâ€: â€œ[0.10, 0.05, 0.80, 0.30, 0]â€,
  â€œa bedâ€: â€œ[0.20, 0.40, 0.60, 0.20, 1]â€,
  â€œa nightstandâ€: â€œ[0.10, 0.60, 0.20, 0.10, 2]â€,
  â€œa lampâ€: â€œ[0.10, 0.70, 0.10, 0.05, 3]â€,
  â€œa rugâ€: â€œ[0.20, 0.68, 0.60, 0.29, 4]â€
Layout Agent
Aesthetics Enhancement agent
â€œa windowâ€: â€œThe large window stretches gracefully across 
the wall, allowing soft natural light to pour in and casting 
gentle shadows.  Its clear glass reflects the peaceful 
outdoors, capturing a moment of calm.â€
â€œa bedâ€: â€œThe bed, at the heart of the room, exudes 
elegance with its crisp white linens that ripple softly in 
the light.  Its minimalist frame adds understated charm, 
offering a haven of comfort and tranquility.â€
â€œa nightstandâ€: â€œThe sleek wooden nightstand, rich in deep 
tones, features a polished surface that catches the soft 
glow of light.â€,
...,
Evaluatorâ€™s Feedback
â€œResultâ€: â€œwrongâ€,
â€œProblemâ€: â€œFirst, the bounding box coordinates of the nightstand and 
lamp need to be specified correctly, as the lamp should be positioned 
above the nightstand according to the output of the Spatial Relations 
Extraction Agent. Additionally, the current layer order of the window 
is incorrect. The window should be positioned further back in the 
scene.â€,
â€œModification Suggestionâ€: â€œAdjust the coordinates of the bounding 
boxes of the lamp and the nightstand to reflect their correct 
positioning. Furthermore, revise the layer order of all objects, 
assigning the window the highest order.â€
Prompt
Set
Input prompt: â€œThe bedroom is bathed in soft natural light.  A large window fills the room, offering a view of the peaceful outdoors.  A 
comfortable bed is placed at the center, dressed in crisp white linens. A sleek wooden nightstand is placed to the left of the bed, with a small 
lamp on top of it, casting a warm glow.  A plush rug on the floor adds texture and warmth, completing the serene and inviting atmosphere.â€
â‘ 
â‘¡
â‘¢
â‘£
â‘¤
â‘¥
â‘ 
â‘¡
â‘¢
â‘£
â‘¤
â‘¨
â‘¦ â‘§
â‘¨
â‘©
â€œa windowâ€: â€œ[0.10, 0.05, 0.80, 0.30, 4]â€,
  â€œa bedâ€: â€œ[0.20, 0.40, 0.60, 0.20, 1]â€,
  â€œa nightstandâ€: â€œ[0.06, 0.70, 0.29, 0.20, 3]â€ï¼Œ
  â€œa lampâ€: â€œ[0.06, 0.59, 0.20, 0.10, 2]â€ï¼Œ
  â€œa rugâ€: â€œ[0.20, 0.68, 0.60, 0.29, 0]â€
Layout Agent
â‘¥â‘¦
Aesthetics Enhancement agent
â€œa windowâ€: â€œThe large window stretches gracefully across the wall, allowing soft natural light 
to pour in and casting gentle shadows.  Its clear glass reflects the peaceful outdoors, 
capturing a moment of calm.â€
â€œa bedâ€: â€œThe bed, at the heart of the room, exudes elegance with its crisp white linens that 
ripple softly in the light.  Its minimalist frame adds understated charm, offering a haven of 
comfort and tranquility.â€
â€œa nightstandâ€: â€œThe sleek wooden nightstand, rich in deep tones, features a polished surface 
that catches the soft glow of light.â€ ,
...,
â‘§
Forward CoT Reasoning
Backward feedback
Evaluatorâ€™s Feedback
Start
â‘©â€œResultâ€: â€œrightâ€,
â€œProblemâ€: â€œâ€,
â€œModification Suggestionâ€: â€œâ€
Figure 6. A case to illustrate the workflow of MCP. In the agentsâ€™ outputs, orange text indicates errors, and green text indicates corrections.
Lee, Yufei Guo, et al. Improving image generation with
better captions. Computer Science. https://cdn. openai.
com/papers/dall-e-3. pdf, 2(3):8, 2023. 1, 2, 6
[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and
Daniel Cohen-Or. Attend-and-excite: Attention-based se-
mantic guidance for text-to-image diffusion models. ACM
Transactions on Graphics (TOG), 42(4):1â€“10, 2023. 6
[4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze
Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo,
Huchuan Lu, et al. Pixart- alpha: Fast training of diffusion
transformer for photorealistic text-to-image synthesis. arXiv
preprint arXiv:2310.00426, 2023. 6
[5] Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng
Gao, and Chunyuan Li. Llava-interactive: An all-in-one
demo for image chat, segmentation, generation and editing.
arXiv preprint arXiv:2311.00571, 2023. 2
[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. Journal of Machine Learning Research, 24(240):
1â€“113, 2023. 2
[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems, 34:8780â€“8794, 2021. 1, 2
[8] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
13
â€œA woman in a pink shirt and jeans holds a white umbrella in the rain.â€
SDXL-base+MCCD (w/ GPT-4o-mini)
+MCCD (w/ GPT-4o)+MCCD (w/ LLaVA-1.5-7b)
â€A glass vase and a metallic watering can are placed beside each other, both filled with colorful flowers.â€
SDXL-base+MCCD (w/ GPT-4o-mini)
+MCCD (w/ GPT-4o)+MCCD (w/ LLaVA-1.5-7b)
Figure 7. Ablation results of MLLMs in MCP.
Figure 8. Additional qualitative analysis.
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image
generation via transformers. Advances in neural information
processing systems, 34:19822â€“19835, 2021. 2
[9] Guian Fang, Zutao Jiang, Jianhua Han, Guangsong Lu, Hang
Xu, and Xiaodan Liang. Boosting text-to-image diffusion
models with fine-grained semantic rewards. arXiv preprint
arXiv:2305.19599, 5, 2023. 1
[10] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun
Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang,
and William Yang Wang. Training-free structured diffusion
guidance for compositional text-to-image synthesis. arXiv
preprint arXiv:2212.05032, 2022. 1, 6
[11] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Ar-
jun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and
William Yang Wang. Layoutgpt: Compositional visual plan-
ning and generation with large language models. Advances
in Neural Information Processing Systems, 36, 2024. 2
[12] Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer,
Salman Khan, and Peter Wonka. Llm blueprint: Enabling
14
text-to-image generation with complex and detailed prompts.
arXiv preprint arXiv:2310.10640, 2023. 2
[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems, 33:6840â€“6851, 2020. 2
[14] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng,
and Gang Yu. Ella: Equip diffusion models with
llm for enhanced semantic alignment. arXiv preprint
arXiv:2403.05135, 2024. 2
[15] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and
Xihui Liu. T2i-compbench: A comprehensive bench-
mark for open-world compositional text-to-image genera-
tion. Advances in Neural Information Processing Systems ,
36:78723â€“78747, 2023. 1, 6, 7
[16] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,
Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, and Shixiang Shane Gu. Aligning text-
to-image models using human feedback. arXiv preprint
arXiv:2302.12192, 2023.
[17] Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han
Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Jun-
feng He, et al. Parrot: Pareto-optimal multi-reward reinforce-
ment learning framework for text-to-image generation. In
European Conference on Computer Vision, pages 462â€“478.
Springer, 2025. 1
[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. In In-
ternational conference on machine learning , pages 19730â€“
19742. PMLR, 2023. 2
[19] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. InPro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 22511â€“22521, 2023. 1, 2
[20] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-
grounded diffusion: Enhancing prompt understanding of
text-to-image diffusion models with large language models.
arXiv preprint arXiv:2305.13655, 2023. 1, 2
[21] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 26296â€“26306, 2024. 11
[22] Mushui Liu, Yuhang Ma, Yang Zhen, Jun Dan, Yunlong
Yu, Zeng Zhao, Zhipeng Hu, Bai Liu, and Changjie Fan.
Llm4gen: Leveraging semantic representation of llms for
text-to-image generation. arXiv preprint arXiv:2407.00737,
2024. 2
[23] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and
Joshua B Tenenbaum. Compositional visual generation with
composable diffusion models. In European Conference on
Computer Vision, pages 423â€“439. Springer, 2022. 1, 6
[24] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian
Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. In Proceedings of the AAAI Conference on
Artificial Intelligence, pages 4296â€“4304, 2024. 2
[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models.arXiv preprint
arXiv:2112.10741, 2021. 2
[26] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M Â¨uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952, 2023. 2, 8
[27] Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-
Seng Chua. Layoutllm-t2i: Eliciting layout guidance from
llm for text-to-image generation. In Proceedings of the 31st
ACM International Conference on Multimedia , pages 643â€“
654, 2023. 1, 2
[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748â€“8763. PMLR, 2021. 2
[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125, 1
(2):3, 2022. 1, 2, 6
[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj Â¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684â€“10695, 2022. 1, 2, 6, 8
[31] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in neural information
processing systems, 35:36479â€“36494, 2022. 1
[32] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International confer-
ence on machine learning, pages 2256â€“2265. PMLR, 2015.
1, 2
[33] Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin,
Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van
Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning text-
to-image generation with image understanding feedback.
In Synthetic Data for Computer Vision Workshop@ CVPR
2024, 2023. 1
[34] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas
Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poul-
ton, Viktor Kerkez, and Robert Stojnic. Galactica:
A large language model for science. arXiv preprint
arXiv:2211.09085, 2022. 2
[35] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wen-
tian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff:
Text-to-image synthesis with training-free box-constrained
diffusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 7452â€“7461, 2023. 4
15
[36] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai
Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagere-
ward: Learning and evaluating human preferences for text-
to-image generation. Advances in Neural Information Pro-
cessing Systems, 36, 2024. 1
[37] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce
Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan,
et al. Baichuan 2: Open large-scale language models. arXiv
preprint arXiv:2309.10305, 2023. 2
[38] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-
sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-
Hsuan Yang. Diffusion models: A comprehensive survey of
methods and applications. ACM Computing Surveys, 56(4):
1â€“39, 2023. 1, 2
[39] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Ste-
fano Ermon, and CUI Bin. Mastering text-to-image dif-
fusion: Recaptioning, planning, and generating with multi-
modal llms. In Forty-first International Conference on Ma-
chine Learning, 2024. 2, 3, 4
[40] Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu,
Minkai Xu, Stefano Ermon, and CUI Bin. Cross-modal con-
textualized diffusion models for text-guided visual genera-
tion and editing. In The Twelfth International Conference on
Learning Representations, 2024. 2
[41] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin
Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael
Zeng, et al. Reco: Region-controlled text-to-image genera-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 14246â€“14255,
2023. 1, 2
[42] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 3836â€“3847, 2023. 2
[43] Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake
Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, and Bin
Cui. Realcompo: Dynamic equilibrium between realism and
compositionality improves text-to-image diffusion models.
arXiv preprint arXiv:2402.12908, 2024. 1
[44] Shanshan Zhong, Zhongzhan Huang, Weushao Wen, Jinghui
Qin, and Liang Lin. Sur-adapter: Enhancing text-to-image
pre-trained diffusion models with large language models. In
Proceedings of the 31st ACM International Conference on
Multimedia, pages 567â€“578, 2023. 2
[45] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592, 2023. 2
16
